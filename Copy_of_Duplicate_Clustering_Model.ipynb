{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajiglover/DS-Projects/blob/main/Copy_of_Duplicate_Clustering_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Duplicate Prediction Model\n"
      ],
      "metadata": {
        "id": "ACWPuMCsWng7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deduplication Optimization âœ¨\n",
        "\n",
        "Today, I'm working on an open-source duplicate prediction model designed to help organizations identify and resolve duplicate records in their entity data. The number of comparisons grows as n(n-1)/2, where n represents the total number of records. My best result so far achieves 34 million comparisons per second on a single commodity GPU. This performance is attained using a TF/IDF matrix with a cosine similarity metric, applied through a nearest-neighbors algorithm. Processing 100,000 records took 144 seconds to fit, which is acceptable. However, scaling up to 1,000,000 records would require over 4 hours and involve half a trillion comparisons.\n",
        "\n",
        "ChatGPT suggests that FAISS from Facebook and Annoy from Spotify might be competitive alternatives. However, given the nature of the n(n-1)/2 comparison growth, I'm not convinced the improvement would be significant. I believe a more promising direction would be to scale the task with Spark. In the meantime, I'm planning to stick with the current algorithm for now and focus on building out some masking for larger datasets.\n",
        "\n",
        "For generating test data with labeled duplicates, I've been using Mimesis, which is quite efficient in creating mock data.\n",
        "\n",
        "Is anyone else working on something similar? I'd love to exchange insights and discuss the best approaches.  Here's a copy of my notebook:\n"
      ],
      "metadata": {
        "id": "E4ciWWn3z8Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mimesis # used for generating test data"
      ],
      "metadata": {
        "id": "aUFCr1OM_91h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data"
      ],
      "metadata": {
        "id": "urQeDlNKXas3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "joUUBubCnEcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Name Variations"
      ],
      "metadata": {
        "id": "BgmwhJFF6mhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_name_variations = {\n",
        "    'Dave': 'David',\n",
        "    'Joe': 'Joseph',\n",
        "    'Elizabeth': 'Betty',\n",
        "    'Bill': 'William',\n",
        "    'Bob': 'Robert',\n",
        "    'Tom': 'Thomas',\n",
        "    'Dick': 'Richard',\n",
        "    'Jim': 'James',\n",
        "    'Jenny': 'Jennifer',\n",
        "    'Kathy': 'Katherine',\n",
        "    'Sue': 'Susan',\n",
        "    'Maggie': 'Margaret',\n",
        "    'Harry': 'Harold',\n",
        "    'Ron': 'Ronald',\n",
        "    'Nick': 'Nicholas',\n",
        "    'Patty': 'Patricia',\n",
        "    'Chris': 'Christopher',\n",
        "    'Mike': 'Michael',\n",
        "    'Steve': 'Steven',\n",
        "    'Andy': 'Andrew',\n",
        "    'Matty': 'Matthew',\n",
        "    'Sam': 'Samuel',\n",
        "    'Tony': 'Anthony',\n",
        "}"
      ],
      "metadata": {
        "id": "zVPgVYRmw-v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_name_variation(full_name, chance=40):\n",
        "    name_parts = full_name.split()\n",
        "    first_name = name_parts[0]\n",
        "    if random.random() < (chance / 100.0):\n",
        "        if first_name in first_name_variations:\n",
        "            first_name = first_name_variations[first_name]\n",
        "        else:\n",
        "            for key, value in first_name_variations.items():\n",
        "                if first_name == value:\n",
        "                    first_name = key\n",
        "                    break\n",
        "    return ' '.join([first_name] + name_parts[1:])\n",
        "\n",
        "print(first_name_variation(\"Joe Satriani\",chance=100))\n",
        "print(first_name_variation(\"David Bowie\",chance=100))"
      ],
      "metadata": {
        "id": "laWrKB3oyKdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last Name Transpositions"
      ],
      "metadata": {
        "id": "aHeL4BiU6soR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def last_name_transposition(full_name, chance=1):\n",
        "    name_parts = full_name.split()\n",
        "    if len(name_parts) > 1 and random.random() < (chance / 100.0):\n",
        "        last_name = name_parts[-1]\n",
        "        if len(last_name) > 4:\n",
        "            i = random.randint(1, len(last_name) - 2)\n",
        "            last_name_chars = list(last_name)\n",
        "            last_name_chars[i], last_name_chars[i+1] = \\\n",
        "              last_name_chars[i+1], last_name_chars[i]\n",
        "            name_parts[-1] = ''.join(last_name_chars)\n",
        "    return ' '.join(name_parts)\n",
        "print(last_name_transposition(\"Jon Anderson\", chance=100))"
      ],
      "metadata": {
        "id": "AO5lXRJN2KGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate N Rows"
      ],
      "metadata": {
        "id": "LZUzpuGoZcIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mimesis import Person, Address, Finance\n",
        "\n",
        "def mimesis_generate_data(n):\n",
        "    data = []\n",
        "    entity_id = 0\n",
        "\n",
        "    person = Person('en')\n",
        "    address_gen = Address('en')\n",
        "    finance = Finance('en')\n",
        "\n",
        "    for _ in range(n):\n",
        "        entity_id += 1\n",
        "        is_person = True\n",
        "        name = person.full_name()\n",
        "        if random.random() < (10.0 / 100.0):\n",
        "            is_person = False\n",
        "            name = finance.company()\n",
        "        street_address = address_gen.address()\n",
        "        city = address_gen.city()\n",
        "        state = address_gen.state(abbr=True)\n",
        "        label = 0  # not a duplicate\n",
        "        data.append([entity_id, name, street_address, city, state, label])\n",
        "        if is_person:\n",
        "            dup_name = first_name_variation(name, chance=30)\n",
        "            dup_name = last_name_transposition(dup_name, chance=1)\n",
        "            if name != dup_name:\n",
        "                entity_id += 1\n",
        "                label = 1\n",
        "                data.append([entity_id, dup_name, street_address, city, state, label])\n",
        "\n",
        "    return data\n",
        "\n",
        "mimesis_mock_data = mimesis_generate_data(100000)\n",
        "mimesis_df = pd.DataFrame(mimesis_mock_data,\n",
        "    columns=['entity_id', 'name', 'address', 'city', 'state', 'label'])\n",
        "#mimesis_df"
      ],
      "metadata": {
        "id": "TKpjq_Q_nKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF/IDF & Nearest Neighbors Cosine\n"
      ],
      "metadata": {
        "id": "ODGN_Lr3Xsbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "df = mimesis_df.sort_values('entity_id', ascending=False).reset_index(drop=True)\n",
        "df['combined'] = df[['name', 'address', 'city', 'state']].agg(' '.join, axis=1)\n",
        "df['dup_id'] = df['entity_id']\n",
        "df['dup_flag'] = 0\n",
        "df['score'] = 0.0\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['combined'])\n",
        "\n",
        "# Use NearestNeighbors to find similarities above the threshold\n",
        "threshold = 0.7\n",
        "nbrs = NearestNeighbors(radius=threshold, metric='cosine').fit(tfidf_matrix)\n",
        "distances, indices = nbrs.radius_neighbors(tfidf_matrix)\n",
        "\n",
        "for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
        "    for d, j in zip(dist, idx):\n",
        "        if i < j:  # only compare with what comes after:\n",
        "            similarity_score = 1 - d  # Convert distance to similarity\n",
        "            if similarity_score > threshold:\n",
        "                df.loc[i, 'dup_flag'] = 1\n",
        "                df.loc[j, 'dup_flag'] = 2\n",
        "                if df.loc[j, 'dup_id'] == df.loc[j, 'entity_id']:\n",
        "                    df.loc[j, 'dup_id'] = df.loc[i, 'entity_id']\n",
        "                    df.loc[j, 'score'] = similarity_score\n"
      ],
      "metadata": {
        "id": "QsS1TfZfg4No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crit = (df['dup_flag'] > 0) | (df['label'] > 0)\n",
        "df[crit][['entity_id','dup_id','dup_flag','score','label',\n",
        "          'name','address','city','state','combined']\n",
        "         ].sort_values(['dup_id','dup_flag'], ascending=[False, True])"
      ],
      "metadata": {
        "id": "xE-rkFRbZOhY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}